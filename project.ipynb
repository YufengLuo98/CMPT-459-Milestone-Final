{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB\n",
    "from imblearn.combine import SMOTEENN \n",
    "from imblearn.pipeline import make_pipeline \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier, XGBRFClassifier, train\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_excel(\"data/cases_2021_train_processed.xlsx\")\n",
    "test_data = pd.read_excel(\"data/cases_2021_test_processed_unlabelled.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2: Mapping the features\n",
    "\n",
    "1. Converted uneccesary float values to integer\n",
    "2. Categorical values that are binary in nature converted to 0's and 1's\n",
    "3. One-hot encoding done on 'province' and 'country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age', 'Confirmed', 'Deaths', 'Recovered', 'Active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[cols] = train_data[cols].apply(pd.to_numeric, downcast='integer', axis=1)\n",
    "test_data[cols] = test_data[cols].apply(pd.to_numeric, downcast='integer', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_groups = {'deceased': 0, 'hospitalized': 1,'nonhospitalized': 2}\n",
    "outcome_groups_inverse = {0: 'deceased', 1: 'hospitalized', 2: 'nonhospitalized'}\n",
    "sex = {'male': 0, 'female': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['outcome_group'] = train_data['outcome_group'].map(outcome_groups)\n",
    "train_data['sex'] = train_data['sex'].map(sex)\n",
    "train_data['province'] = train_data['province'].fillna('Philippines')\n",
    "train_data['chronic_disease_binary'] = train_data['chronic_disease_binary'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['sex'] = test_data['sex'].map(sex)\n",
    "test_data['province'] = test_data['province'].fillna('Philippines')\n",
    "test_data['chronic_disease_binary'] = test_data['chronic_disease_binary'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['outcome_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cols = ['province', 'country']\n",
    "train_data = pd.get_dummies(train_data, columns=dummy_cols)\n",
    "test_data = pd.get_dummies(test_data, columns=dummy_cols)\n",
    "\n",
    "# Need to make sure the columns are the same in train and test data\n",
    "test_data = test_data.reindex(columns=train_data.columns, fill_value=0)\n",
    "test_data.drop('outcome_group', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['date_confirmation_int'] = train_data['date_confirmation'].dt.strftime(\"%Y%m%d\").astype(int)\n",
    "test_data['date_confirmation_int'] = test_data['date_confirmation'].dt.strftime(\"%Y%m%d\").astype(int)\n",
    "train_data.drop(['date_confirmation', 'Confirmed', 'Deaths', 'Recovered', 'Active'], axis=1, inplace=True)\n",
    "test_data.drop(['date_confirmation', 'Confirmed', 'Deaths', 'Recovered', 'Active'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop('outcome_group', axis=1)\n",
    "y = train_data['outcome_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printClassificationResults(models):\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        ensemble_predictions = pd.DataFrame(index=range(y_test.shape[0]))\n",
    "        \n",
    "        for i in range(len(models)):\n",
    "            model = models[i]\n",
    "            model.fit(X_train, y_train)\n",
    "            ensemble_predictions[i] = model.predict(X_test)\n",
    "\n",
    "        predictions = ensemble_predictions.mode(axis=1)[0].astype(int)\n",
    "        all_labels = all_labels + list(y_test)\n",
    "        all_predictions = all_predictions + list(predictions)\n",
    "\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSubmissionFile(models, filename):\n",
    "    ensemble_predictions = pd.DataFrame(index=range(test_data.shape[0]))\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        model.fit(X, y)\n",
    "        ensemble_predictions[i] = model.predict(test_data)\n",
    "\n",
    "    predictions = ensemble_predictions.mode(axis=1)[0].astype(int)\n",
    "    submission = pd.DataFrame({'Prediction': predictions})\n",
    "    submission.index.name = 'Id'\n",
    "    submission.to_csv('submissions/{filename}'.format(filename=filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Random Forest Best params: max_depth=14, max_samples=0.56, min_samples_leaf=1, min_samples_split=5, class_weight='balanced', random_state=1\n",
    "params = {\n",
    "    \"randomforestclassifier__max_depth\": [i for i in range(12, 15)],\n",
    "    \"randomforestclassifier__min_samples_split\": [i for i in range(3, 10, 2)],\n",
    "    \"randomforestclassifier__min_samples_leaf\": [i for i in range(1, 3)],\n",
    "    'randomforestclassifier__max_samples': [i/100 for i in range(50, 70, 3)]\n",
    "}\n",
    "random_forest = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=14, max_samples=0.56, min_samples_leaf=1, min_samples_split=5, class_weight='balanced', random_state=1))\n",
    "printClassificationResults([random_forest])\n",
    "createSubmissionFile([random_forest], 'random_forest.csv')\n",
    "\n",
    "#search = GridSearchCV(random_forest, params, scoring='f1_macro')\n",
    "#search.fit(X, y)\n",
    "#print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "#print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Neural Network\n",
    "neural_network = make_pipeline(StandardScaler(), MLPClassifier(hidden_layer_sizes=(50,), max_iter=500, random_state=1))\n",
    "printClassificationResults([neural_network])\n",
    "createSubmissionFile([neural_network], 'neural_network.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing XGBoost\n",
    "xgboost = make_pipeline(StandardScaler(), XGBRFClassifier(use_label_encoder=False, max_depth=15, eval_metric=\"mlogloss\"))\n",
    "printClassificationResults([xgboost])\n",
    "createSubmissionFile([xgboost], 'xgboost.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Enseble of Random Forest, Neural Network, XGBoost\n",
    "printClassificationResults([random_forest, neural_network, xgboost])\n",
    "createSubmissionFile([random_forest, neural_network, xgboost], 'ensemble.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ff4c835d7956487bba13277bf340d67efa161ba50aa68448d8213de87c08323"
  },
  "kernelspec": {
   "display_name": "CMPT-353",
   "language": "python",
   "name": "cmpt-353"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
